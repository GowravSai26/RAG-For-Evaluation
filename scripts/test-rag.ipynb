{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1daa738",
   "metadata": {},
   "source": [
    "# RAG Pipeline Testing Notebook\n",
    "\n",
    "This comprehensive notebook implements and tests the complete RAG (Retrieval-Augmented Generation) pipeline using the `search_or_ingest` endpoint. The notebook contains all necessary components including:\n",
    "\n",
    "- FAISS vector store implementation\n",
    "- Document ingestion and processing\n",
    "- Text chunking and embedding generation\n",
    "- Semantic search and retrieval\n",
    "- The unified search_or_ingest endpoint functionality\n",
    "\n",
    "## Overview\n",
    "\n",
    "The RAG pipeline follows this flow:\n",
    "1. **Document Ingestion**: Parse and chunk documents (PDF, DOCX, TXT)\n",
    "2. **Embedding Generation**: Create vector embeddings using SentenceTransformer models\n",
    "3. **FAISS Indexing**: Store embeddings in a FAISS index for efficient search\n",
    "4. **Query Processing**: Convert search queries into embeddings\n",
    "5. **Similarity Search**: Find relevant document chunks using vector similarity\n",
    "6. **Result Ranking**: Apply MMR (Maximal Marginal Relevance) for diverse results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ceb3f6",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47f77caa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'frontend'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseModel, Field, validator\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Document processing\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfitz\u001b[39;00m  \u001b[38;5;66;03m# PyMuPDF for PDF reading\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Machine learning and embeddings\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Github\\RAG-For-Evaluation\\venv\\Lib\\site-packages\\fitz\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfrontend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtools\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mop\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'frontend'"
     ]
    }
   ],
   "source": [
    "# Core Python libraries\n",
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import threading\n",
    "import tempfile\n",
    "import uuid\n",
    "import hashlib\n",
    "import re\n",
    "import string\n",
    "import zipfile\n",
    "import xml.etree.ElementTree as ET\n",
    "from dataclasses import asdict, dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Sequence, Tuple, Iterator, Any, Union, Iterable\n",
    "\n",
    "# Data manipulation and numerical computing\n",
    "import numpy as np\n",
    "\n",
    "# FastAPI and Pydantic for API schemas\n",
    "from pydantic import BaseModel, Field, validator\n",
    "\n",
    "# Document processing\n",
    "import fitz  # PyMuPDF for PDF reading\n",
    "\n",
    "# Machine learning and embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# File locking (platform-specific)\n",
    "if os.name == \"nt\":\n",
    "    import msvcrt\n",
    "else:\n",
    "    import fcntl\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b833169d",
   "metadata": {},
   "source": [
    "## 2. Configuration and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5903a468",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Configuration - copied from core/config.py\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m INDEX_ROOT = \u001b[43mPath\u001b[49m(os.getenv(\u001b[33m\"\u001b[39m\u001b[33mINDEX_ROOT\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstorage\u001b[39m\u001b[33m\"\u001b[39m)).resolve()\n\u001b[32m      3\u001b[39m EMBEDDING_MODEL = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mEMBEDDING_MODEL\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mall-MiniLM-L6-v2\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m CHUNK_SIZE = \u001b[38;5;28mint\u001b[39m(os.getenv(\u001b[33m\"\u001b[39m\u001b[33mCHUNK_SIZE\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m1000\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "# Configuration - copied from core/config.py\n",
    "INDEX_ROOT = Path(os.getenv(\"INDEX_ROOT\", \"storage\")).resolve()\n",
    "EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"all-MiniLM-L6-v2\")\n",
    "CHUNK_SIZE = int(os.getenv(\"CHUNK_SIZE\", \"1000\"))\n",
    "CHUNK_OVERLAP = int(os.getenv(\"CHUNK_OVERLAP\", \"200\"))\n",
    "\n",
    "# Thread configuration for embedding models\n",
    "def configure_thread_limits():\n",
    "    \"\"\"Configure thread limits for embedding model processing.\"\"\"\n",
    "    raw = os.getenv(\"EMBEDDING_MAX_THREADS\") or os.getenv(\"EMBEDDING_THREADS\") or \"2\"\n",
    "    try:\n",
    "        desired = int(raw)\n",
    "    except ValueError:\n",
    "        return\n",
    "    if desired <= 0:\n",
    "        return\n",
    "    \n",
    "    thread_envs = (\n",
    "        \"OMP_NUM_THREADS\",\n",
    "        \"OPENBLAS_NUM_THREADS\", \n",
    "        \"MKL_NUM_THREADS\",\n",
    "        \"NUMEXPR_NUM_THREADS\",\n",
    "        \"VECLIB_MAXIMUM_THREADS\",\n",
    "        \"BLIS_NUM_THREADS\",\n",
    "    )\n",
    "    \n",
    "    for env_name in thread_envs:\n",
    "        os.environ.setdefault(env_name, str(desired))\n",
    "    os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "    \n",
    "    try:\n",
    "        import torch\n",
    "        torch.set_num_threads(desired)\n",
    "        torch.set_num_interop_threads(max(1, desired // 2))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Apply thread configuration\n",
    "configure_thread_limits()\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"  INDEX_ROOT: {INDEX_ROOT}\")\n",
    "print(f\"  EMBEDDING_MODEL: {EMBEDDING_MODEL}\")\n",
    "print(f\"  CHUNK_SIZE: {CHUNK_SIZE}\")\n",
    "print(f\"  CHUNK_OVERLAP: {CHUNK_OVERLAP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90023cc6",
   "metadata": {},
   "source": [
    "## 3. Core Data Structures and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f3b50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data structures from vectorstore/faiss_store2.py\n",
    "@dataclass(slots=True)\n",
    "class ChunkMetadata:\n",
    "    \"\"\"Metadata captured for every chunk stored in the index.\"\"\"\n",
    "    client_id: str\n",
    "    file_id: str\n",
    "    chunk_id: int\n",
    "    page_no: Optional[int] = None\n",
    "    filename: Optional[str] = None\n",
    "    file_path_abs: Optional[str] = None\n",
    "    file_hash: Optional[str] = None\n",
    "    extra: Dict[str, str] = field(default_factory=dict)\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        self.client_id = sys.intern(self.client_id)\n",
    "        self.file_id = sys.intern(self.file_id)\n",
    "        if self.file_path_abs is not None:\n",
    "            self.file_path_abs = sys.intern(self.file_path_abs)\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        data = asdict(self)\n",
    "        if not data[\"extra\"]:\n",
    "            data.pop(\"extra\")\n",
    "        return data\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, payload: Dict) -> \"ChunkMetadata\":\n",
    "        extra = payload.get(\"extra\") or {}\n",
    "        return cls(\n",
    "            client_id=payload[\"client_id\"],\n",
    "            file_id=payload[\"file_id\"],\n",
    "            chunk_id=payload[\"chunk_id\"],\n",
    "            page_no=payload.get(\"page_no\"),\n",
    "            filename=payload.get(\"filename\"),\n",
    "            file_path_abs=payload.get(\"file_path_abs\"),\n",
    "            file_hash=payload.get(\"file_hash\"),\n",
    "            extra=extra,\n",
    "        )\n",
    "\n",
    "@dataclass\n",
    "class StoredChunk:\n",
    "    \"\"\"Intermediate representation of a chunk prior to persistence.\"\"\"\n",
    "    text: str\n",
    "    metadata: ChunkMetadata\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            \"text\": self.text,\n",
    "            \"metadata\": self.metadata.to_dict(),\n",
    "        }\n",
    "\n",
    "@dataclass(slots=True)\n",
    "class ChunkRecord:\n",
    "    \"\"\"Lightweight record pointing to chunk text stored on disk.\"\"\"\n",
    "    metadata: ChunkMetadata\n",
    "    offset: int\n",
    "    length: int\n",
    "    docstore: \"DocStore\"\n",
    "\n",
    "    @property\n",
    "    def text(self) -> str:\n",
    "        return self.docstore.read_text(self)\n",
    "\n",
    "    def to_index_entry(self) -> Dict:\n",
    "        return {\n",
    "            \"metadata\": self.metadata.to_dict(),\n",
    "            \"offset\": self.offset,\n",
    "            \"length\": self.length,\n",
    "        }\n",
    "\n",
    "@dataclass\n",
    "class EmbeddingBatch:\n",
    "    \"\"\"Helper bundle returned when encoding document chunks.\"\"\"\n",
    "    chunk_indices: List[int]\n",
    "    vectors: np.ndarray\n",
    "\n",
    "print(\"Core data structures defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2f988e",
   "metadata": {},
   "source": [
    "## 4. Text Processing and Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402d7e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text processing functions from core/text_clean.py\n",
    "_PDF_JUNK_RE = re.compile(\n",
    "    r\"(\\b\\d+\\s+0\\s+obj\\b|/URI\\(|/S/GoTo\\b|endobj\\b|stream\\b|endstream\\b)\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Sanitize input text by removing PDF artefacts and normalizing whitespace.\"\"\"\n",
    "    cleaned = _PDF_JUNK_RE.sub(\" \", text)\n",
    "    cleaned = re.sub(r\"\\s+\", \" \", cleaned)\n",
    "    return cleaned.strip()\n",
    "\n",
    "def is_valid_chunk(text: str, min_printable_ratio: float = 0.3) -> bool:\n",
    "    \"\"\"Determine if a chunk of text is likely to contain meaningful content.\"\"\"\n",
    "    if not text:\n",
    "        return False\n",
    "    printable = set(string.printable)\n",
    "    printable_count = sum(1 for ch in text if ch in printable)\n",
    "    ratio = printable_count / len(text)\n",
    "    return ratio >= min_printable_ratio\n",
    "\n",
    "# Chunking functions from core/chunker.py\n",
    "def chunk_text(text: str, *, max_length: int = 1500, overlap: int = 200) -> List[str]:\n",
    "    \"\"\"Split a text into overlapping chunks of at most `max_length` characters.\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    chunks: List[str] = []\n",
    "    start = 0\n",
    "    length = len(text)\n",
    "    max_length = max(1, max_length)\n",
    "    overlap = max(0, overlap)\n",
    "    \n",
    "    while start < length:\n",
    "        end = min(length, start + max_length)\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        if end == length:\n",
    "            break\n",
    "        start = max(0, end - overlap)\n",
    "    return chunks\n",
    "\n",
    "# Hashing function from core/hashing.py\n",
    "def file_sha256(path: Union[str, Path], chunk_size: int = 8192) -> str:\n",
    "    \"\"\"Compute the SHA-256 hash of a file.\"\"\"\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        while True:\n",
    "            data = f.read(chunk_size)\n",
    "            if not data:\n",
    "                break\n",
    "            h.update(data)\n",
    "    return h.hexdigest()\n",
    "\n",
    "print(\"Text processing utilities defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8070d9e0",
   "metadata": {},
   "source": [
    "## 5. Document Readers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64abde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document reader functions from core/readers.py\n",
    "def read_pdf(path: Path) -> Iterator[Tuple[int, str]]:\n",
    "    \"\"\"Iterate over pages in a PDF and yield their text.\"\"\"\n",
    "    doc = fitz.open(str(path))\n",
    "    try:\n",
    "        for i, page in enumerate(doc):\n",
    "            page_no = i + 1\n",
    "            text = page.get_text(\"text\") or \"\"\n",
    "            yield page_no, text\n",
    "    finally:\n",
    "        doc.close()\n",
    "\n",
    "def read_txt(path: Path) -> Iterator[Tuple[Optional[int], str]]:\n",
    "    \"\"\"Read a plain text file and return its content.\"\"\"\n",
    "    text = Path(path).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    yield None, text\n",
    "\n",
    "def read_docx(path: Path) -> Iterator[Tuple[Optional[int], str]]:\n",
    "    \"\"\"Extract text from a DOCX file without external dependencies.\"\"\"\n",
    "    with zipfile.ZipFile(path) as z:\n",
    "        namespace = {\n",
    "            \"w\": \"http://schemas.openxmlformats.org/wordprocessingml/2006/main\"\n",
    "        }\n",
    "        with z.open(\"word/document.xml\") as doc_xml:\n",
    "            tree = ET.parse(doc_xml)\n",
    "        root = tree.getroot()\n",
    "        paragraphs: List[str] = []\n",
    "        \n",
    "        for p in root.iter(f\"{{{namespace['w']}}}p\"):\n",
    "            texts: List[str] = []\n",
    "            for r in p.iter(f\"{{{namespace['w']}}}t\"):\n",
    "                if r.text:\n",
    "                    texts.append(r.text)\n",
    "            if texts:\n",
    "                paragraphs.append(\"\".join(texts))\n",
    "    text = \"\\n\\n\".join(paragraphs)\n",
    "    yield None, text\n",
    "\n",
    "print(\"Document readers defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8743a0",
   "metadata": {},
   "source": [
    "## 6. File Locking and Directory Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3414b486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File utilities from vectorstore/faiss_store2.py\n",
    "def _fsync_directory(path: Path) -> None:\n",
    "    \"\"\"Best-effort directory fsync; no-op on platforms that do not support it.\"\"\"\n",
    "    if os.name == \"nt\":\n",
    "        return\n",
    "    try:\n",
    "        fd = os.open(str(path), os.O_RDONLY)\n",
    "    except OSError:\n",
    "        return\n",
    "    try:\n",
    "        os.fsync(fd)\n",
    "    except OSError:\n",
    "        pass\n",
    "    finally:\n",
    "        os.close(fd)\n",
    "\n",
    "class FileLock:\n",
    "    \"\"\"Cross-platform file locking context manager.\"\"\"\n",
    "    \n",
    "    def __init__(self, lock_file_path: Path, timeout: float = 5.0):\n",
    "        self.lock_file_path = lock_file_path\n",
    "        self.timeout = timeout\n",
    "        self._lock_file = None\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self.lock_file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        self._lock_file = open(self.lock_file_path, 'w')\n",
    "        \n",
    "        start_time = time.time()\n",
    "        while time.time() - start_time < self.timeout:\n",
    "            try:\n",
    "                if os.name == 'nt':  # Windows\n",
    "                    msvcrt.locking(self._lock_file.fileno(), msvcrt.LK_NBLCK, 1)\n",
    "                else:  # Unix/Linux\n",
    "                    fcntl.flock(self._lock_file.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)\n",
    "                return self\n",
    "            except (OSError, IOError):\n",
    "                time.sleep(0.1)\n",
    "        \n",
    "        self._lock_file.close()\n",
    "        raise RuntimeError(f\"Could not acquire lock on {self.lock_file_path} within {self.timeout}s\")\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        if self._lock_file:\n",
    "            try:\n",
    "                if os.name == 'nt':  # Windows\n",
    "                    msvcrt.locking(self._lock_file.fileno(), msvcrt.LK_UNLCK, 1)\n",
    "                else:  # Unix/Linux\n",
    "                    fcntl.flock(self._lock_file.fileno(), fcntl.LOCK_UN)\n",
    "            except (OSError, IOError):\n",
    "                pass\n",
    "            finally:\n",
    "                self._lock_file.close()\n",
    "                try:\n",
    "                    self.lock_file_path.unlink()\n",
    "                except FileNotFoundError:\n",
    "                    pass\n",
    "\n",
    "print(\"File locking utilities defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e2fbc0",
   "metadata": {},
   "source": [
    "## 7. Embedding Backend and Model Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952ebdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model management and embedding backend from vectorstore/faiss_store2.py\n",
    "class ModelLoadError(RuntimeError):\n",
    "    \"\"\"Raised when the embedding model cannot be loaded.\"\"\"\n",
    "\n",
    "_MODEL_CACHE: Dict[str, SentenceTransformer] = {}\n",
    "_MODEL_LOCK = threading.RLock()\n",
    "\n",
    "def _get_or_create_model(model_name: str) -> SentenceTransformer:\n",
    "    \"\"\"Return a cached SentenceTransformer instance for the given model.\"\"\"\n",
    "    with _MODEL_LOCK:\n",
    "        model = _MODEL_CACHE.get(model_name)\n",
    "        if model is None:\n",
    "            try:\n",
    "                model = SentenceTransformer(model_name)\n",
    "            except (OSError, RuntimeError) as exc:\n",
    "                raise ModelLoadError(\n",
    "                    f\"Unable to load embedding model '{model_name}'. Ensure the model is available offline or provide a local path via EMBEDDING_MODEL.\"\n",
    "                ) from exc\n",
    "            _MODEL_CACHE[model_name] = model\n",
    "        return model\n",
    "\n",
    "def _as_float32(matrix: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Convert matrix to float32 if not already.\"\"\"\n",
    "    if matrix.dtype == np.float32:\n",
    "        return matrix\n",
    "    return matrix.astype(np.float32)\n",
    "\n",
    "def encode_texts(\n",
    "    texts: Sequence[str],\n",
    "    *,\n",
    "    model_name: str,\n",
    "    batch_size: int = 128,\n",
    "    normalize: bool = True,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Encode a batch of texts using a shared SentenceTransformer instance.\"\"\"\n",
    "    if not texts:\n",
    "        return np.empty((0, 0), dtype=np.float32)\n",
    "    model = _get_or_create_model(model_name)\n",
    "    vectors = model.encode(\n",
    "        list(texts),\n",
    "        batch_size=batch_size,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=normalize,\n",
    "        show_progress_bar=False,\n",
    "    )\n",
    "    return _as_float32(vectors)\n",
    "\n",
    "def encode_text(\n",
    "    text: str,\n",
    "    *,\n",
    "    model_name: str,\n",
    "    batch_size: int = 128,\n",
    "    normalize: bool = True,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Encode a single text while leveraging the shared model cache.\"\"\"\n",
    "    vectors = encode_texts([text], model_name=model_name, batch_size=batch_size, normalize=normalize)\n",
    "    if vectors.ndim == 1:\n",
    "        return vectors.astype(np.float32, copy=False)\n",
    "    if vectors.size == 0:\n",
    "        return np.zeros((0,), dtype=np.float32)\n",
    "    return vectors[0]\n",
    "\n",
    "class SentenceTransformerBackend:\n",
    "    \"\"\"Thin convenience wrapper over the cached SentenceTransformer.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        *,\n",
    "        batch_size: int = 128,\n",
    "        normalize_embeddings: bool = True,\n",
    "    ) -> None:\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.normalize_embeddings = normalize_embeddings\n",
    "\n",
    "    @property\n",
    "    def embedding_dimension(self) -> int:\n",
    "        model = _get_or_create_model(self.model_name)\n",
    "        return model.get_sentence_embedding_dimension()\n",
    "\n",
    "    def encode_documents(self, texts: Sequence[str]) -> np.ndarray:\n",
    "        return encode_texts(\n",
    "            texts,\n",
    "            model_name=self.model_name,\n",
    "            batch_size=self.batch_size,\n",
    "            normalize=self.normalize_embeddings,\n",
    "        )\n",
    "\n",
    "    def encode_query(self, text: str) -> np.ndarray:\n",
    "        return encode_text(\n",
    "            text,\n",
    "            model_name=self.model_name,\n",
    "            batch_size=self.batch_size,\n",
    "            normalize=self.normalize_embeddings,\n",
    "        )\n",
    "\n",
    "print(\"Embedding backend defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3adec55",
   "metadata": {},
   "source": [
    "## 8. Document Store Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb7a489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DocStore class from vectorstore/faiss_store2.py\n",
    "class DocStore:\n",
    "    \"\"\"Streams chunk text from a binary file while keeping metadata in an index.\"\"\"\n",
    "\n",
    "    DATA_FILENAME = \"docstore.bin\"\n",
    "    INDEX_FILENAME = \"docstore_index.json\"\n",
    "\n",
    "    def __init__(self, root: Path) -> None:\n",
    "        self.root = Path(root)\n",
    "        self.data_path = self.root / self.DATA_FILENAME\n",
    "        self.index_path = self.root / self.INDEX_FILENAME\n",
    "        self._records: List[ChunkRecord] = []\n",
    "        self._by_file: Dict[Tuple[str, str], List[int]] = {}\n",
    "\n",
    "    @property\n",
    "    def records(self) -> List[ChunkRecord]:\n",
    "        return self._records\n",
    "\n",
    "    @property\n",
    "    def by_file(self) -> Dict[Tuple[str, str], List[int]]:\n",
    "        return self._by_file\n",
    "\n",
    "    def load(self) -> None:\n",
    "        if not self.index_path.exists():\n",
    "            self._records = []\n",
    "            self._by_file = {}\n",
    "            return\n",
    "        data = json.loads(self.index_path.read_text(encoding=\"utf-8\"))\n",
    "        self._records = [\n",
    "            ChunkRecord(\n",
    "                metadata=ChunkMetadata.from_dict(entry[\"metadata\"]),\n",
    "                offset=int(entry[\"offset\"]),\n",
    "                length=int(entry[\"length\"]),\n",
    "                docstore=self,\n",
    "            )\n",
    "            for entry in data.get(\"entries\", [])\n",
    "        ]\n",
    "        self._by_file = {\n",
    "            tuple(key.split(\"::\")): list(indices)\n",
    "            for key, indices in data.get(\"by_file\", {}).items()\n",
    "        }\n",
    "\n",
    "    def append(self, chunks: Sequence[StoredChunk]) -> List[ChunkRecord]:\n",
    "        if not chunks:\n",
    "            return []\n",
    "            \n",
    "        # Use process-level file locking to prevent corruption\n",
    "        lock_path = self.root / \".docstore.lock\"\n",
    "        with FileLock(lock_path):\n",
    "            self.root.mkdir(parents=True, exist_ok=True)\n",
    "            offset = self.data_path.stat().st_size if self.data_path.exists() else 0\n",
    "            new_records: List[ChunkRecord] = []\n",
    "            with open(self.data_path, \"ab\") as handle:\n",
    "                for chunk in chunks:\n",
    "                    encoded = chunk.text.encode(\"utf-8\")\n",
    "                    length = len(encoded)\n",
    "                    handle.write(encoded)\n",
    "                    record = ChunkRecord(chunk.metadata, offset, length, self)\n",
    "                    self._records.append(record)\n",
    "                    new_records.append(record)\n",
    "                    key = (chunk.metadata.client_id, chunk.metadata.file_id)\n",
    "                    self._by_file.setdefault(key, []).append(len(self._records) - 1)\n",
    "                    offset += length\n",
    "                handle.flush()\n",
    "                os.fsync(handle.fileno())  # Ensure data is written\n",
    "            self._write_index()\n",
    "            return new_records\n",
    "\n",
    "    def read_text(self, record: ChunkRecord) -> str:\n",
    "        if record.length == 0:\n",
    "            return \"\"\n",
    "        with open(self.data_path, \"rb\") as handle:\n",
    "            handle.seek(record.offset)\n",
    "            data = handle.read(record.length)\n",
    "        return data.decode(\"utf-8\")\n",
    "\n",
    "    def _write_index(self) -> None:\n",
    "        payload = {\n",
    "            \"entries\": [record.to_index_entry() for record in self._records],\n",
    "            \"by_file\": {\"::\".join(key): indices for key, indices in self._by_file.items()},\n",
    "        }\n",
    "        \n",
    "        # Use atomic write: temp file + rename\n",
    "        with tempfile.NamedTemporaryFile(\n",
    "            mode='w', \n",
    "            encoding='utf-8', \n",
    "            dir=self.index_path.parent,\n",
    "            delete=False,\n",
    "            suffix='.tmp'\n",
    "        ) as temp_file:\n",
    "            json.dump(payload, temp_file, ensure_ascii=False)\n",
    "            temp_file.flush()\n",
    "            os.fsync(temp_file.fileno())  # Force to disk\n",
    "            temp_path = Path(temp_file.name)\n",
    "        \n",
    "        # Atomic rename - this is the commit point\n",
    "        temp_path.replace(self.index_path)\n",
    "        _fsync_directory(self.index_path.parent)\n",
    "\n",
    "print(\"DocStore implementation defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a5fe76",
   "metadata": {},
   "source": [
    "## 9. Embedding Store Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c935ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EmbeddingStore class from vectorstore/faiss_store2.py\n",
    "class EmbeddingStore:\n",
    "    \"\"\"File-backed embedding matrix managed in append-only fashion.\"\"\"\n",
    "\n",
    "    META_FILENAME = \"embeddings.meta.json\"\n",
    "    DATA_FILENAME = \"embeddings.f32\"\n",
    "\n",
    "    def __init__(self, root: Path):\n",
    "        self.root = Path(root)\n",
    "        self.meta_path = self.root / self.META_FILENAME\n",
    "        self.data_path = self.root / self.DATA_FILENAME\n",
    "        self._lock = threading.RLock()\n",
    "        self._dimension: Optional[int] = None\n",
    "        self._count: int = 0\n",
    "\n",
    "    def index_exists(self) -> bool:\n",
    "        return self.meta_path.exists() and self.data_path.exists()\n",
    "\n",
    "    def load(self) -> None:\n",
    "        with self._lock:\n",
    "            self.root.mkdir(parents=True, exist_ok=True)\n",
    "            if self.meta_path.exists():\n",
    "                meta = json.loads(self.meta_path.read_text(encoding=\"utf-8\"))\n",
    "                self._dimension = meta.get(\"dimension\")\n",
    "                if self._dimension is not None:\n",
    "                    self._dimension = int(self._dimension)\n",
    "                self._count = int(meta.get(\"count\", 0))\n",
    "            else:\n",
    "                self._dimension = None\n",
    "                self._count = 0\n",
    "\n",
    "    @property\n",
    "    def dimension(self) -> Optional[int]:\n",
    "        return self._dimension\n",
    "\n",
    "    @property\n",
    "    def count(self) -> int:\n",
    "        return self._count\n",
    "\n",
    "    def _write_meta(self) -> None:\n",
    "        payload = {\"dimension\": self._dimension, \"count\": self._count}\n",
    "        \n",
    "        # Use atomic write: temp file + rename\n",
    "        with tempfile.NamedTemporaryFile(\n",
    "            mode='w', \n",
    "            encoding='utf-8', \n",
    "            dir=self.meta_path.parent,\n",
    "            delete=False,\n",
    "            suffix='.tmp'\n",
    "        ) as temp_file:\n",
    "            json.dump(payload, temp_file)\n",
    "            temp_file.flush()\n",
    "            os.fsync(temp_file.fileno())  # Force to disk\n",
    "            temp_path = Path(temp_file.name)\n",
    "        \n",
    "        # Atomic rename\n",
    "        temp_path.replace(self.meta_path)\n",
    "        _fsync_directory(self.meta_path.parent)\n",
    "\n",
    "    def append(self, vectors: np.ndarray, *, start_index: int) -> None:\n",
    "        vectors = _as_float32(np.asarray(vectors))\n",
    "        if vectors.ndim != 2:\n",
    "            raise ValueError(\"Expected a 2D array of embeddings.\")\n",
    "        rows, dim = vectors.shape\n",
    "        \n",
    "        # Use process-level file locking for embeddings\n",
    "        lock_path = self.root / \".embeddings.lock\"\n",
    "        with FileLock(lock_path):\n",
    "            with self._lock:\n",
    "                if self._dimension is None:\n",
    "                    self._dimension = dim\n",
    "                    self._write_meta()\n",
    "                elif dim != self._dimension:\n",
    "                    raise ValueError(\n",
    "                        f\"Embedding dimension mismatch: store={self._dimension}, batch={dim}.\"\n",
    "                    )\n",
    "                if start_index != self._count:\n",
    "                    raise ValueError(\n",
    "                        f\"Embeddings must be appended sequentially: start_index={start_index}, current_count={self._count}.\"\n",
    "                    )\n",
    "                if rows == 0:\n",
    "                    return\n",
    "                with self.data_path.open(\"ab\") as handle:\n",
    "                    handle.write(np.ascontiguousarray(vectors).tobytes())\n",
    "                    handle.flush()\n",
    "                    os.fsync(handle.fileno())  # Ensure data is written\n",
    "                self._count += rows\n",
    "                self._write_meta()\n",
    "\n",
    "    def fetch(self, indices: Sequence[int]) -> np.ndarray:\n",
    "        if not indices:\n",
    "            dim = self._dimension or 0\n",
    "            return np.empty((0, dim), dtype=np.float32)\n",
    "        if self._dimension is None or self._count == 0:\n",
    "            raise ValueError(\"Embedding store is empty.\")\n",
    "        dim = self._dimension\n",
    "        stride = dim * 4\n",
    "        result = np.empty((len(indices), dim), dtype=np.float32)\n",
    "        with open(self.data_path, \"rb\") as handle:\n",
    "            for out_pos, idx in enumerate(indices):\n",
    "                if idx < 0 or idx >= self._count:\n",
    "                    raise IndexError(f\"Embedding index {idx} out of range\")\n",
    "                handle.seek(idx * stride)\n",
    "                buffer = handle.read(stride)\n",
    "                if len(buffer) != stride:\n",
    "                    raise ValueError(\"Unexpected end of embedding file.\")\n",
    "                result[out_pos] = np.frombuffer(buffer, dtype=np.float32)\n",
    "        return result\n",
    "\n",
    "print(\"EmbeddingStore implementation defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fe6000",
   "metadata": {},
   "source": [
    "## 10. Worker Synchronization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ae642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worker synchronization from vectorstore/faiss_store2.py\n",
    "class WorkerSemaphore:\n",
    "    \"\"\"Simple file-based semaphore for worker coordination.\"\"\"\n",
    "    \n",
    "    def __init__(self, semaphore_dir: Path, max_workers: int = 1):\n",
    "        self.semaphore_dir = semaphore_dir\n",
    "        self.max_workers = max_workers\n",
    "        self.semaphore_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def acquire(self, timeout: float = 30.0) -> bool:\n",
    "        \"\"\"Acquire a worker slot.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        while time.time() - start_time < timeout:\n",
    "            # Count existing worker files\n",
    "            worker_files = list(self.semaphore_dir.glob(\"worker_*.pid\"))\n",
    "            if len(worker_files) < self.max_workers:\n",
    "                # Try to create our worker file\n",
    "                worker_file = self.semaphore_dir / f\"worker_{os.getpid()}.pid\"\n",
    "                try:\n",
    "                    with open(worker_file, 'x') as f:  # 'x' fails if file exists\n",
    "                        f.write(str(os.getpid()))\n",
    "                    return True\n",
    "                except FileExistsError:\n",
    "                    pass\n",
    "            time.sleep(0.1)\n",
    "        return False\n",
    "    \n",
    "    def release(self):\n",
    "        \"\"\"Release our worker slot.\"\"\"\n",
    "        worker_file = self.semaphore_dir / f\"worker_{os.getpid()}.pid\"\n",
    "        try:\n",
    "            worker_file.unlink()\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "print(\"Worker synchronization utilities defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737984f9",
   "metadata": {},
   "source": [
    "## 11. FAISS Vector Store Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4e0b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VectorStore class from vectorstore/faiss_store2.py\n",
    "class VectorStore:\n",
    "    \"\"\"Persistent chunk catalog with efficient (client_id, file_id) indexing.\"\"\"\n",
    "\n",
    "    DOCSTORE_SUBDIR = \"docstore\"\n",
    "\n",
    "    def __init__(self, index_dir: Path):\n",
    "        self.index_dir = Path(index_dir)\n",
    "        self.docstore = DocStore(self.index_dir / self.DOCSTORE_SUBDIR)\n",
    "        self._embedding_store = EmbeddingStore(self.index_dir)\n",
    "        self._lock = threading.RLock()\n",
    "        self._loaded = False\n",
    "        self._records: List[ChunkRecord] = []\n",
    "        self._by_file: Dict[Tuple[str, str], List[int]] = {}\n",
    "        self._worker_semaphore = WorkerSemaphore(self.index_dir / \".workers\", max_workers=1)\n",
    "\n",
    "    def index_exists(self) -> bool:\n",
    "        return self.docstore.index_path.exists()\n",
    "\n",
    "    def load(self) -> None:\n",
    "        with self._lock:\n",
    "            if self._loaded:\n",
    "                return\n",
    "            self._ensure_storage_dir()\n",
    "            self.docstore.load()\n",
    "            self._records = list(self.docstore.records)\n",
    "            self._by_file = {key: list(indices) for key, indices in self.docstore.by_file.items()}\n",
    "            self._embedding_store.load()\n",
    "            doc_count = len(self._records)\n",
    "            emb_count = self._embedding_store.count\n",
    "            if emb_count not in (0, doc_count):\n",
    "                raise ValueError(\n",
    "                    f\"Embedding store row count {emb_count} does not match docstore count {doc_count}.\"\n",
    "                )\n",
    "            self._loaded = True\n",
    "\n",
    "    def _ensure_storage_dir(self) -> None:\n",
    "        self.index_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.docstore.root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def append_chunks(self, chunks: Sequence[StoredChunk], *, embeddings: Optional[np.ndarray] = None) -> None:\n",
    "        if not chunks:\n",
    "            return\n",
    "        if embeddings is None:\n",
    "            raise ValueError(\"Embeddings must be provided when appending chunks; call encode_new_chunks first.\")\n",
    "        embeddings = _as_float32(np.asarray(embeddings))\n",
    "        if embeddings.ndim != 2 or embeddings.shape[0] != len(chunks):\n",
    "            raise ValueError(\"Embeddings row count must match number of chunks.\")\n",
    "\n",
    "        # Acquire worker semaphore to prevent concurrent operations\n",
    "        if not self._worker_semaphore.acquire():\n",
    "            raise RuntimeError(\"Could not acquire worker slot - too many concurrent operations\")\n",
    "\n",
    "        try:\n",
    "            with self._lock:\n",
    "                self._ensure_storage_dir()\n",
    "                start_idx = len(self._records)\n",
    "                was_loaded = self._loaded\n",
    "                previous_records = list(self._records)\n",
    "                previous_by_file = {key: list(indices) for key, indices in self._by_file.items()}\n",
    "                previous_docstore_records = list(self.docstore.records)\n",
    "                previous_docstore_by_file = {\n",
    "                    key: list(indices) for key, indices in self.docstore.by_file.items()\n",
    "                }\n",
    "                previous_data_size = (\n",
    "                    self.docstore.data_path.stat().st_size\n",
    "                    if self.docstore.data_path.exists()\n",
    "                    else 0\n",
    "                )\n",
    "                data_path_existed = self.docstore.data_path.exists()\n",
    "                docstore_appended = False\n",
    "\n",
    "                try:\n",
    "                    new_records = self.docstore.append(chunks)\n",
    "                    docstore_appended = bool(new_records)\n",
    "                    self._records = list(self.docstore.records)\n",
    "                    self._by_file = {\n",
    "                        key: list(indices) for key, indices in self.docstore.by_file.items()\n",
    "                    }\n",
    "                    self._embedding_store.append(embeddings, start_index=start_idx)\n",
    "                    self._loaded = True\n",
    "                except Exception:\n",
    "                    current_data_exists = self.docstore.data_path.exists()\n",
    "                    current_size = (\n",
    "                        self.docstore.data_path.stat().st_size if current_data_exists else 0\n",
    "                    )\n",
    "                    records_changed = len(self.docstore.records) != len(previous_docstore_records)\n",
    "                    data_changed = (\n",
    "                        current_data_exists != data_path_existed or current_size != previous_data_size\n",
    "                    )\n",
    "                    docstore_mutated = docstore_appended or records_changed or data_changed\n",
    "\n",
    "                    if docstore_mutated:\n",
    "                        # Roll back docstore mutations to keep stores consistent.\n",
    "                        if current_data_exists:\n",
    "                            with open(self.docstore.data_path, \"ab\") as handle:\n",
    "                                handle.truncate(previous_data_size)\n",
    "                                handle.flush()\n",
    "                                os.fsync(handle.fileno())\n",
    "                        if not data_path_existed:\n",
    "                            try:\n",
    "                                self.docstore.data_path.unlink()\n",
    "                            except FileNotFoundError:\n",
    "                                pass\n",
    "                        if previous_docstore_records:\n",
    "                            self.docstore._records = list(previous_docstore_records)\n",
    "                            self.docstore._by_file = {\n",
    "                                key: list(indices) for key, indices in previous_docstore_by_file.items()\n",
    "                            }\n",
    "                            self.docstore._write_index()\n",
    "                        else:\n",
    "                            self.docstore._records = []\n",
    "                            self.docstore._by_file = {}\n",
    "                            try:\n",
    "                                self.docstore.index_path.unlink()\n",
    "                            except FileNotFoundError:\n",
    "                                pass\n",
    "                        self._records = list(previous_records)\n",
    "                        self._by_file = {\n",
    "                            key: list(indices) for key, indices in previous_by_file.items()\n",
    "                        }\n",
    "                        self._loaded = was_loaded\n",
    "                    raise\n",
    "        finally:\n",
    "            self._worker_semaphore.release()\n",
    "\n",
    "    def iter_chunks(self, *, client_id: Optional[str] = None, file_id: Optional[str] = None) -> Iterable[ChunkRecord]:\n",
    "        self.load()\n",
    "        if client_id is None and file_id is None:\n",
    "            yield from self._records\n",
    "            return\n",
    "        if client_id is None or file_id is None:\n",
    "            raise ValueError(\"Both client_id and file_id are required for filtered iteration.\")\n",
    "        for idx in self._by_file.get((client_id, file_id), []):\n",
    "            yield self._records[idx]\n",
    "\n",
    "    def chunk_indices(self, client_id: str, file_id: str) -> List[int]:\n",
    "        self.load()\n",
    "        return list(self._by_file.get((client_id, file_id), []))\n",
    "\n",
    "    def chunk_count(self, *, client_id: Optional[str] = None, file_id: Optional[str] = None) -> int:\n",
    "        self.load()\n",
    "        if client_id is None and file_id is None:\n",
    "            return len(self._records)\n",
    "        return len(self.chunk_indices(client_id, file_id))\n",
    "\n",
    "    @classmethod\n",
    "    def get_shared_store(cls, index_root: Path) -> \"VectorStore\":\n",
    "        return cls(Path(index_root) / \"shared\")\n",
    "\n",
    "print(\"VectorStore core implementation defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3955ff",
   "metadata": {},
   "source": [
    "## 12. Vector Store Search and Embedding Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4534dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add search and embedding methods to VectorStore\n",
    "def encode_file_chunks(\n",
    "    self,\n",
    "    *,\n",
    "    client_id: str,\n",
    "    file_id: str,\n",
    "    embedder: SentenceTransformerBackend,\n",
    ") -> EmbeddingBatch:\n",
    "    self.load()\n",
    "    indices = self.chunk_indices(client_id, file_id)\n",
    "    if not indices:\n",
    "        dim = self._embedding_store.dimension or embedder.embedding_dimension\n",
    "        return EmbeddingBatch([], np.empty((0, dim), dtype=np.float32))\n",
    "    try:\n",
    "        vectors = self._embedding_store.fetch(indices)\n",
    "        if vectors.shape[0] == len(indices):\n",
    "            return EmbeddingBatch(indices, vectors)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    texts = [self._records[idx].text for idx in indices]\n",
    "    vectors = embedder.encode_documents(texts)\n",
    "    return EmbeddingBatch(indices, _as_float32(vectors))\n",
    "\n",
    "def encode_new_chunks(\n",
    "    self,\n",
    "    chunks: Sequence[StoredChunk],\n",
    "    *,\n",
    "    embedder: SentenceTransformerBackend,\n",
    ") -> EmbeddingBatch:\n",
    "    self.load()\n",
    "    if not chunks:\n",
    "        dim = self._embedding_store.dimension or embedder.embedding_dimension\n",
    "        return EmbeddingBatch([], np.empty((0, dim), dtype=np.float32))\n",
    "    existing_dim = self._embedding_store.dimension\n",
    "    if existing_dim is not None and existing_dim != embedder.embedding_dimension:\n",
    "        raise ValueError(\n",
    "            f\"Embedder dimension {embedder.embedding_dimension} does not match stored dimension {existing_dim}.\"\n",
    "        )\n",
    "    texts = [chunk.text for chunk in chunks]\n",
    "    vectors = _as_float32(embedder.encode_documents(texts))\n",
    "    start_idx = len(self._records)\n",
    "    indices = list(range(start_idx, start_idx + len(chunks)))\n",
    "    return EmbeddingBatch(indices, vectors)\n",
    "\n",
    "def semantic_search_filtered(\n",
    "    self,\n",
    "    *,\n",
    "    query: str,\n",
    "    client_id: str,\n",
    "    file_id: str,\n",
    "    embedder: SentenceTransformerBackend,\n",
    "    k: int = 5,\n",
    "    mmr: bool = False,\n",
    "    fetch_k: int = 25,\n",
    "    lambda_mult: float = 0.5,\n",
    ") -> List[Dict]:\n",
    "    batch = self.encode_file_chunks(client_id=client_id, file_id=file_id, embedder=embedder)\n",
    "    if not batch.chunk_indices:\n",
    "        return []\n",
    "    if not isinstance(k, int) or k <= 0:\n",
    "        k = 5\n",
    "    fetch_k = max(fetch_k or 0, k)\n",
    "    query_vec = _as_float32(embedder.encode_query(query))\n",
    "    scores = np.dot(batch.vectors, query_vec)\n",
    "    if scores.size == 0:\n",
    "        return []\n",
    "    sorted_positions = np.argsort(-scores)\n",
    "    candidate_positions = sorted_positions[:fetch_k]\n",
    "    candidate_scores = scores[candidate_positions]\n",
    "    candidate_vectors = batch.vectors[candidate_positions]\n",
    "    if mmr and len(candidate_positions) > k:\n",
    "        selected_relative = self._apply_mmr(\n",
    "            candidate_vectors,\n",
    "            candidate_scores,\n",
    "            k,\n",
    "            lambda_mult,\n",
    "        )\n",
    "        final_positions = [candidate_positions[pos] for pos in selected_relative]\n",
    "    else:\n",
    "        final_positions = list(candidate_positions[:k])\n",
    "    final_scores = [float(scores[pos]) for pos in final_positions]\n",
    "    final_indices = [batch.chunk_indices[pos] for pos in final_positions]\n",
    "    return self._build_results(final_indices, final_scores)\n",
    "\n",
    "def _build_results(self, indices: Sequence[int], scores: Sequence[float]) -> List[Dict]:\n",
    "    results: List[Dict] = []\n",
    "    for idx, score in zip(indices, scores):\n",
    "        record = self._records[idx]\n",
    "        result = {\n",
    "            \"excerpt\": record.text,\n",
    "            \"page_no\": record.metadata.page_no,\n",
    "            \"file_id\": record.metadata.file_id,\n",
    "            \"filename\": record.metadata.filename,\n",
    "            \"score\": float(score),\n",
    "            \"metadata\": record.metadata.to_dict(),\n",
    "        }\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "def _apply_mmr(\n",
    "    self,\n",
    "    candidate_vectors: np.ndarray,\n",
    "    candidate_scores: np.ndarray,\n",
    "    k: int,\n",
    "    lambda_mult: float,\n",
    ") -> List[int]:\n",
    "    selected: List[int] = []\n",
    "    available = list(range(candidate_vectors.shape[0]))\n",
    "    lambda_mult = float(lambda_mult)\n",
    "    while available and len(selected) < k:\n",
    "        best_idx = None\n",
    "        best_score = -np.inf\n",
    "        for pos in available:\n",
    "            relevance = float(candidate_scores[pos])\n",
    "            if not selected:\n",
    "                diversity = 0.0\n",
    "            else:\n",
    "                selected_vecs = candidate_vectors[selected]\n",
    "                diversity = float(np.max(np.dot(selected_vecs, candidate_vectors[pos])))\n",
    "            mmr_score = lambda_mult * relevance - (1.0 - lambda_mult) * diversity\n",
    "            if mmr_score > best_score:\n",
    "                best_score = mmr_score\n",
    "                best_idx = pos\n",
    "        if best_idx is None:\n",
    "            best_idx = available[0]\n",
    "        selected.append(best_idx)\n",
    "        available.remove(best_idx)\n",
    "    return selected\n",
    "\n",
    "# Add methods to VectorStore class\n",
    "VectorStore.encode_file_chunks = encode_file_chunks\n",
    "VectorStore.encode_new_chunks = encode_new_chunks\n",
    "VectorStore.semantic_search_filtered = semantic_search_filtered\n",
    "VectorStore._build_results = _build_results\n",
    "VectorStore._apply_mmr = _apply_mmr\n",
    "\n",
    "print(\"Vector store search methods added successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff2233d",
   "metadata": {},
   "source": [
    "## 13. API Schema Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2a5821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Schema definitions from app/api_schemas.py\n",
    "class SearchOrIngestFile(BaseModel):\n",
    "    \"\"\"Single file for search or ingest operation.\"\"\"\n",
    "    file_id: str = Field(..., description=\"Unique identifier for the file.\")\n",
    "    file_type: str = Field(..., description=\"Type of the file (pdf, docx, or txt). Case-insensitive.\")\n",
    "    file_path: str = Field(..., description=\"Absolute or relative filesystem path to the file.\")\n",
    "    filename: Optional[str] = Field(None, description=\"Optional human-readable filename for display purposes.\")\n",
    "\n",
    "    @validator('file_type')\n",
    "    def validate_file_type(cls, v):\n",
    "        normalized = v.strip().lower()\n",
    "        if normalized not in {\"pdf\", \"docx\", \"txt\"}:\n",
    "            raise ValueError(f\"Unsupported file_type '{v}'. Must be one of: pdf, docx, txt.\")\n",
    "        return normalized\n",
    "\n",
    "class SearchOrIngestRequest(BaseModel):\n",
    "    \"\"\"Request body for the unified /search_or_ingest endpoint.\"\"\"\n",
    "    client_id: str = Field(..., description=\"Client identifier for data isolation.\")\n",
    "    files: List[SearchOrIngestFile] = Field(..., description=\"List of files to search or ingest.\")\n",
    "    question: str = Field(..., description=\"Natural language query to search for.\")\n",
    "    top_k: Optional[int] = Field(5, description=\"Maximum number of results to return.\")\n",
    "    mmr: Optional[bool] = Field(False, description=\"Whether to apply Maximal Marginal Relevance.\")\n",
    "    search_type: Optional[str] = Field(\"semantic\", description=\"Type of search: 'tfidf' or 'semantic'.\")\n",
    "\n",
    "class RetrieveResult(BaseModel):\n",
    "    \"\"\"Single retrieval result entry.\"\"\"\n",
    "    excerpt: str\n",
    "    page_no: Optional[int]\n",
    "    file_id: str\n",
    "    filename: Optional[str] = None\n",
    "    score: float\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "class SearchOrIngestFileResult(BaseModel):\n",
    "    \"\"\"Result for a single file in search or ingest operation.\"\"\"\n",
    "    file_id: str\n",
    "    filename: Optional[str] = None\n",
    "    ingestion_status: str  # \"already_existed\", \"newly_ingested\", \"failed\"\n",
    "    chunks_indexed: int\n",
    "    error_message: Optional[str] = None\n",
    "\n",
    "class SearchOrIngestResponse(BaseModel):\n",
    "    \"\"\"Response body for the unified /search_or_ingest endpoint.\"\"\"\n",
    "    client_id: str\n",
    "    files_processed: List[SearchOrIngestFileResult]\n",
    "    search_results: List[RetrieveResult]\n",
    "    total_results: int\n",
    "\n",
    "class FileProcessResult(BaseModel):\n",
    "    \"\"\"Result for a single file in batch ingestion.\"\"\"\n",
    "    file_id: str\n",
    "    status: str  # \"ingested\", \"skipped\", \"failed\"\n",
    "    chunks_indexed: int\n",
    "    error_message: Optional[str] = None\n",
    "\n",
    "print(\"API schema definitions loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa3458b",
   "metadata": {},
   "source": [
    "## 14. Document Ingestion Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecb75e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document ingestion functions from app/routers/ingest.py\n",
    "def validate_file_type(file_type: str, for_batch: bool = False) -> str:\n",
    "    \"\"\"Validate and normalize file type.\"\"\"\n",
    "    normalized = file_type.strip().lower()\n",
    "    if normalized not in {\"pdf\", \"docx\", \"txt\"}:\n",
    "        error_msg = f\"Unsupported file_type '{file_type}'. Must be one of: pdf, docx, txt.\"\n",
    "        if for_batch:\n",
    "            raise ValueError(error_msg)\n",
    "        else:\n",
    "            raise Exception(error_msg)\n",
    "    return normalized\n",
    "\n",
    "def validate_file_path(file_path: str, file_type: str, for_batch: bool = False) -> Path:\n",
    "    \"\"\"Validate file path and extension match.\"\"\"\n",
    "    file_path_obj = Path(file_path).expanduser().resolve()\n",
    "    \n",
    "    # Validate existence\n",
    "    if not file_path_obj.exists():\n",
    "        error_msg = f\"File not found: {file_path}\"\n",
    "        if for_batch:\n",
    "            raise ValueError(error_msg)\n",
    "        else:\n",
    "            raise Exception(error_msg)\n",
    "    \n",
    "    # Validate extension matches declared type\n",
    "    ext = file_path_obj.suffix.lower()\n",
    "    expected_ext = {\"pdf\": \".pdf\", \"docx\": \".docx\", \"txt\": \".txt\"}[file_type]\n",
    "    if ext != expected_ext:\n",
    "        error_msg = f\"File extension '{ext}' does not match declared file_type '{file_type}'.\"\n",
    "        if for_batch:\n",
    "            raise ValueError(error_msg)\n",
    "        else:\n",
    "            raise Exception(error_msg)\n",
    "    \n",
    "    return file_path_obj\n",
    "\n",
    "def check_existing_chunks(store: VectorStore, client_id: str, file_id: str, file_hash: str) -> List[Dict]:\n",
    "    \"\"\"Check if chunks already exist for this client+file+hash combination.\"\"\"\n",
    "    if not store.index_exists():\n",
    "        return []\n",
    "    matches: List[Dict] = []\n",
    "    try:\n",
    "        for chunk in store.iter_chunks(client_id=client_id, file_id=file_id):\n",
    "            if chunk.metadata.file_hash == file_hash:\n",
    "                matches.append({\"metadata\": chunk.metadata.to_dict()})\n",
    "    except ValueError:\n",
    "        # No chunks recorded for this (client, file) pair yet\n",
    "        return []\n",
    "    return matches\n",
    "\n",
    "def extract_chunks_from_file(file_path_obj: Path, file_type: str, client_id: str, file_id: str, file_hash: str, filename: str):\n",
    "    \"\"\"Yield processed chunk dictionaries from a file.\"\"\"\n",
    "    chunk_counter = 0\n",
    "\n",
    "    if file_type == \"pdf\":\n",
    "        reader = read_pdf(file_path_obj)\n",
    "    elif file_type == \"docx\":\n",
    "        reader = read_docx(file_path_obj)\n",
    "    else:\n",
    "        reader = read_txt(file_path_obj)\n",
    "\n",
    "    for page_no, raw_text in reader:\n",
    "        cleaned = clean_text(raw_text)\n",
    "        if not is_valid_chunk(cleaned):\n",
    "            continue\n",
    "\n",
    "        sub_chunks = chunk_text(cleaned, max_length=CHUNK_SIZE, overlap=CHUNK_OVERLAP)\n",
    "        for sub in sub_chunks:\n",
    "            if len(sub) < 50:\n",
    "                continue\n",
    "\n",
    "            yield {\n",
    "                \"text\": sub,\n",
    "                \"metadata\": {\n",
    "                    \"client_id\": client_id,\n",
    "                    \"file_id\": file_id,\n",
    "                    \"filename\": filename,\n",
    "                    \"file_path_abs\": str(file_path_obj),\n",
    "                    \"file_hash\": file_hash,\n",
    "                    \"page_no\": page_no,\n",
    "                    \"chunk_id\": chunk_counter,\n",
    "                },\n",
    "            }\n",
    "            chunk_counter += 1\n",
    "\n",
    "def convert_to_stored_chunks(chunks: List[Dict]) -> List[StoredChunk]:\n",
    "    \"\"\"Convert list of chunk dictionaries to StoredChunk instances.\"\"\"\n",
    "    stored: List[StoredChunk] = []\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        meta = chunk.get(\"metadata\", {})\n",
    "        chunk_id = meta.get(\"chunk_id\", idx)\n",
    "        extra = {\n",
    "            key: value\n",
    "            for key, value in meta.items()\n",
    "            if key\n",
    "            not in {\"client_id\", \"file_id\", \"chunk_id\", \"page_no\", \"filename\", \"file_path_abs\", \"file_hash\"}\n",
    "        }\n",
    "        metadata = ChunkMetadata(\n",
    "            client_id=meta[\"client_id\"],\n",
    "            file_id=meta[\"file_id\"],\n",
    "            chunk_id=int(chunk_id),\n",
    "            page_no=meta.get(\"page_no\"),\n",
    "            filename=meta.get(\"filename\"),\n",
    "            file_path_abs=meta.get(\"file_path_abs\"),\n",
    "            file_hash=meta.get(\"file_hash\"),\n",
    "            extra=extra,\n",
    "        )\n",
    "        stored.append(StoredChunk(text=chunk[\"text\"], metadata=metadata))\n",
    "    return stored\n",
    "\n",
    "print(\"Document ingestion functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb58445",
   "metadata": {},
   "source": [
    "## 15. File Processing and Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2689f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedder(model_name: str) -> SentenceTransformerBackend:\n",
    "    \"\"\"Instantiate a backend that shares the cached SentenceTransformer.\"\"\"\n",
    "    return SentenceTransformerBackend(model_name=model_name)\n",
    "\n",
    "def save_chunks_to_store(store: VectorStore, chunks_iterable, file_id: str, file_type: str, file_path_obj: Path, file_hash: str) -> int:\n",
    "    \"\"\"Stream chunks into the vector store, returning the number ingested.\"\"\"\n",
    "    os.makedirs(store.index_dir, exist_ok=True)\n",
    "    embedder = get_embedder(EMBEDDING_MODEL)\n",
    "    total = 0\n",
    "    buffer: List[Dict] = []\n",
    "    BATCH_SIZE = 64\n",
    "\n",
    "    def flush_buffer() -> int:\n",
    "        if not buffer:\n",
    "            return 0\n",
    "        stored = convert_to_stored_chunks(buffer)\n",
    "        buffer.clear()\n",
    "        batch = store.encode_new_chunks(stored, embedder=embedder)\n",
    "        count = len(stored)\n",
    "        try:\n",
    "            store.append_chunks(stored, embeddings=batch.vectors)\n",
    "        finally:\n",
    "            del stored\n",
    "            del batch\n",
    "        return count\n",
    "\n",
    "    try:\n",
    "        for chunk in chunks_iterable:\n",
    "            buffer.append(chunk)\n",
    "            if len(buffer) >= BATCH_SIZE:\n",
    "                total += flush_buffer()\n",
    "        total += flush_buffer()\n",
    "    except ModelLoadError as exc:\n",
    "        raise RuntimeError(str(exc)) from exc\n",
    "    finally:\n",
    "        buffer.clear()\n",
    "    return total\n",
    "\n",
    "def process_single_file(\n",
    "    client_id: str,\n",
    "    file_id: str,\n",
    "    file_type: str,\n",
    "    file_path: str,\n",
    "    store: VectorStore,\n",
    "    for_batch: bool = False,\n",
    "    filename: str | None = None,\n",
    ") -> FileProcessResult:\n",
    "    \"\"\"Process a single file for ingestion.\"\"\"\n",
    "    try:\n",
    "        normalized_file_type = validate_file_type(file_type, for_batch=for_batch)\n",
    "        file_path_obj = validate_file_path(file_path, normalized_file_type, for_batch=for_batch)\n",
    "        file_hash = file_sha256(file_path_obj)\n",
    "\n",
    "        existing_chunks = check_existing_chunks(store, client_id, file_id, file_hash)\n",
    "        if existing_chunks:\n",
    "            return FileProcessResult(\n",
    "                file_id=file_id,\n",
    "                status=\"skipped\",\n",
    "                chunks_indexed=len(existing_chunks),\n",
    "                error_message=None,\n",
    "            )\n",
    "\n",
    "        # Extract and process chunks\n",
    "        chunks_iterable = extract_chunks_from_file(\n",
    "            file_path_obj, normalized_file_type, client_id, file_id, file_hash, filename or file_id\n",
    "        )\n",
    "        \n",
    "        # Convert to list for processing\n",
    "        chunks_list = list(chunks_iterable)\n",
    "        \n",
    "        if not chunks_list:\n",
    "            return FileProcessResult(\n",
    "                file_id=file_id,\n",
    "                status=\"failed\",\n",
    "                chunks_indexed=0,\n",
    "                error_message=\"No valid chunks extracted from file\",\n",
    "            )\n",
    "        \n",
    "        # Save to store\n",
    "        chunks_indexed = save_chunks_to_store(store, chunks_list, file_id, normalized_file_type, file_path_obj, file_hash)\n",
    "        \n",
    "        # Reload store to reflect changes\n",
    "        if hasattr(store, \"_loaded\"):\n",
    "            store._loaded = False\n",
    "        store.load()\n",
    "        \n",
    "        return FileProcessResult(\n",
    "            file_id=file_id,\n",
    "            status=\"ingested\",\n",
    "            chunks_indexed=chunks_indexed,\n",
    "            error_message=None,\n",
    "        )\n",
    "\n",
    "    except Exception as exc:\n",
    "        return FileProcessResult(\n",
    "            file_id=file_id,\n",
    "            status=\"failed\",\n",
    "            chunks_indexed=0,\n",
    "            error_message=str(exc),\n",
    "        )\n",
    "\n",
    "print(\"File processing functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34a660e",
   "metadata": {},
   "source": [
    "## 16. Search or Ingest Endpoint Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8424f28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_or_ingest(request: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Unified endpoint that processes multiple files: ingests if needed, then searches across all.\n",
    "    \n",
    "    This is the main function from app/routers/search_or_ingest.py\n",
    "    \n",
    "    Flow:\n",
    "    1. For each file: check if already ingested using client_id + file_id + file_hash\n",
    "    2. If not ingested: ingest the file\n",
    "    3. Perform search across all successfully ingested files\n",
    "    4. Return combined results from all files\n",
    "    \"\"\"\n",
    "    # Parse request\n",
    "    client_id = request[\"client_id\"]\n",
    "    files = request[\"files\"]\n",
    "    question = request[\"question\"]\n",
    "    top_k = request.get(\"top_k\", 5)\n",
    "    mmr = request.get(\"mmr\", False)\n",
    "    \n",
    "    # Use shared vector store\n",
    "    store = VectorStore.get_shared_store(INDEX_ROOT)\n",
    "    \n",
    "    # Process each file\n",
    "    files_processed = []\n",
    "    successfully_ingested_files = []\n",
    "    \n",
    "    for file_req in files:\n",
    "        try:\n",
    "            # Process single file (ingest if needed)\n",
    "            ingest_result = process_single_file(\n",
    "                client_id=client_id,\n",
    "                file_id=file_req[\"file_id\"],\n",
    "                file_type=file_req[\"file_type\"],\n",
    "                file_path=file_req[\"file_path\"],\n",
    "                store=store,\n",
    "                for_batch=True,\n",
    "                filename=file_req.get(\"filename\")\n",
    "            )\n",
    "            \n",
    "            # Map ingest result status to search_or_ingest status\n",
    "            if ingest_result.status == \"ingested\":\n",
    "                ingestion_status = \"newly_ingested\"\n",
    "                successfully_ingested_files.append(file_req[\"file_id\"])\n",
    "            elif ingest_result.status == \"skipped\":\n",
    "                ingestion_status = \"already_existed\"\n",
    "                successfully_ingested_files.append(file_req[\"file_id\"])\n",
    "            else:\n",
    "                ingestion_status = \"failed\"\n",
    "            \n",
    "            files_processed.append({\n",
    "                \"file_id\": file_req[\"file_id\"],\n",
    "                \"filename\": file_req.get(\"filename\") or file_req[\"file_id\"],\n",
    "                \"ingestion_status\": ingestion_status,\n",
    "                \"chunks_indexed\": ingest_result.chunks_indexed,\n",
    "                \"error_message\": ingest_result.error_message\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            files_processed.append({\n",
    "                \"file_id\": file_req[\"file_id\"],\n",
    "                \"filename\": file_req.get(\"filename\") or file_req[\"file_id\"],\n",
    "                \"ingestion_status\": \"failed\",\n",
    "                \"chunks_indexed\": 0,\n",
    "                \"error_message\": str(e)\n",
    "            })\n",
    "    \n",
    "    # Now perform search across all successfully ingested files\n",
    "    all_search_results = []\n",
    "    \n",
    "    if successfully_ingested_files:\n",
    "        try:\n",
    "            # Load the store if not already loaded\n",
    "            if store.index_exists():\n",
    "                store.load()\n",
    "                \n",
    "                # Search across all successfully ingested files (semantic search)\n",
    "                embedder = SentenceTransformerBackend(model_name=EMBEDDING_MODEL)\n",
    "                for file_id in successfully_ingested_files:\n",
    "                    try:\n",
    "                        search_results_raw = store.semantic_search_filtered(\n",
    "                            query=question,\n",
    "                            client_id=client_id,\n",
    "                            file_id=file_id,\n",
    "                            embedder=embedder,\n",
    "                            k=top_k or 5,\n",
    "                            mmr=bool(mmr),\n",
    "                        )\n",
    "                    except ModelLoadError as exc:\n",
    "                        raise Exception(str(exc)) from exc\n",
    "                    \n",
    "                    # Convert to result format\n",
    "                    file_results = []\n",
    "                    for res in search_results_raw:\n",
    "                        file_results.append({\n",
    "                            \"excerpt\": res[\"excerpt\"],\n",
    "                            \"page_no\": res[\"page_no\"],\n",
    "                            \"file_id\": res[\"file_id\"],\n",
    "                            \"filename\": res[\"filename\"],\n",
    "                            \"score\": res[\"score\"],\n",
    "                            \"metadata\": res[\"metadata\"]\n",
    "                        })\n",
    "                    all_search_results.extend(file_results)\n",
    "                \n",
    "                # Sort combined results by score (highest first) and limit to top_k\n",
    "                all_search_results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "                all_search_results = all_search_results[:top_k or 5]\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Search failed: {e}\")\n",
    "            # Search failed, but we still return the ingestion results\n",
    "            pass\n",
    "    \n",
    "    return {\n",
    "        \"client_id\": client_id,\n",
    "        \"files_processed\": files_processed,\n",
    "        \"search_results\": all_search_results,\n",
    "        \"total_results\": len(all_search_results)\n",
    "    }\n",
    "\n",
    "print(\"Search or ingest endpoint implementation defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fec11c",
   "metadata": {},
   "source": [
    "## 17. Testing and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0c8f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test document\n",
    "def create_test_document():\n",
    "    \"\"\"Create a sample text document for testing.\"\"\"\n",
    "    test_content = \"\"\"\n",
    "    Workplace Mental Health Guidelines\n",
    "    \n",
    "    Our company recognizes that workplace mental health is a critical concern that affects employee well-being, \n",
    "    productivity, and overall organizational success. We are committed to creating a supportive environment \n",
    "    where employees feel safe to discuss mental health challenges and seek appropriate support.\n",
    "    \n",
    "    Key Principles:\n",
    "    1. Mental health is as important as physical health\n",
    "    2. All employees deserve support and understanding\n",
    "    3. We promote a culture of openness and acceptance\n",
    "    4. Professional resources are available to all staff\n",
    "    \n",
    "    Support Resources:\n",
    "    - Employee Assistance Program (EAP)\n",
    "    - Mental health first aid training\n",
    "    - Flexible work arrangements\n",
    "    - Stress management workshops\n",
    "    - Confidential counseling services\n",
    "    \n",
    "    The company actively supports employee well-being through various initiatives including:\n",
    "    - Regular mental health awareness campaigns\n",
    "    - Training for managers on mental health support\n",
    "    - Creating psychologically safe work environments\n",
    "    - Providing access to mental health professionals\n",
    "    \n",
    "    We believe that by prioritizing mental health, we create a more inclusive, productive, and positive workplace \n",
    "    for all employees. Our commitment extends to ongoing evaluation and improvement of our mental health policies \n",
    "    and support systems.\n",
    "    \"\"\"\n",
    "    \n",
    "    test_file_path = INDEX_ROOT / \"test_document.txt\"\n",
    "    test_file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    test_file_path.write_text(test_content, encoding='utf-8')\n",
    "    return test_file_path\n",
    "\n",
    "# Create the test document\n",
    "test_file = create_test_document()\n",
    "print(f\"Test document created at: {test_file}\")\n",
    "print(f\"Document exists: {test_file.exists()}\")\n",
    "\n",
    "# Test the search_or_ingest function\n",
    "def test_search_or_ingest():\n",
    "    \"\"\"Test the complete search_or_ingest pipeline.\"\"\"\n",
    "    \n",
    "    # Test request\n",
    "    request = {\n",
    "        \"client_id\": \"test_client_notebook\", \n",
    "        \"files\": [{\n",
    "            \"file_id\": \"test_doc_001\",\n",
    "            \"file_type\": \"txt\",\n",
    "            \"file_path\": str(test_file),\n",
    "            \"filename\": \"workplace_mental_health.txt\"\n",
    "        }],\n",
    "        \"question\": \"Does the company acknowledge workplace mental health as an important concern?\",\n",
    "        \"top_k\": 3,\n",
    "        \"mmr\": False\n",
    "    }\n",
    "    \n",
    "    print(\"Testing search_or_ingest endpoint...\")\n",
    "    print(f\"Request: {json.dumps(request, indent=2)}\")\n",
    "    \n",
    "    try:\n",
    "        response = search_or_ingest(request)\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"RESPONSE:\")\n",
    "        print(\"=\"*50)\n",
    "        print(json.dumps(response, indent=2))\n",
    "        \n",
    "        # Validate response\n",
    "        assert \"client_id\" in response\n",
    "        assert \"files_processed\" in response\n",
    "        assert \"search_results\" in response\n",
    "        assert \"total_results\" in response\n",
    "        \n",
    "        # Check file processing\n",
    "        files_processed = response[\"files_processed\"]\n",
    "        assert len(files_processed) == 1\n",
    "        file_result = files_processed[0]\n",
    "        assert file_result[\"file_id\"] == \"test_doc_001\"\n",
    "        assert file_result[\"ingestion_status\"] in [\"newly_ingested\", \"already_existed\"]\n",
    "        assert file_result[\"chunks_indexed\"] > 0\n",
    "        \n",
    "        # Check search results\n",
    "        search_results = response[\"search_results\"]\n",
    "        assert len(search_results) > 0\n",
    "        assert response[\"total_results\"] == len(search_results)\n",
    "        \n",
    "        # Check result content\n",
    "        for result in search_results:\n",
    "            assert \"excerpt\" in result\n",
    "            assert \"file_id\" in result\n",
    "            assert \"filename\" in result\n",
    "            assert \"score\" in result\n",
    "            assert \"metadata\" in result\n",
    "            assert result[\"file_id\"] == \"test_doc_001\"\n",
    "        \n",
    "        print(\"\\n All tests passed!\")\n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n Test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Run the test\n",
    "test_result = test_search_or_ingest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d49bba9",
   "metadata": {},
   "source": [
    "## 18. Additional Test Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7544159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with existing document (should skip ingestion)\n",
    "def test_existing_document():\n",
    "    \"\"\"Test the endpoint with a document that's already been ingested.\"\"\"\n",
    "    \n",
    "    request = {\n",
    "        \"client_id\": \"test_client_notebook\",\n",
    "        \"files\": [{\n",
    "            \"file_id\": \"test_doc_001\",  # Same file ID as before\n",
    "            \"file_type\": \"txt\", \n",
    "            \"file_path\": str(test_file),\n",
    "            \"filename\": \"workplace_mental_health.txt\"\n",
    "        }],\n",
    "        \"question\": \"What support resources does the company provide?\",\n",
    "        \"top_k\": 5,\n",
    "        \"mmr\": True  # Test MMR feature\n",
    "    }\n",
    "    \n",
    "    print(\"Testing with existing document (should skip ingestion)...\")\n",
    "    \n",
    "    try:\n",
    "        response = search_or_ingest(request)\n",
    "        print(f\"Ingestion status: {response['files_processed'][0]['ingestion_status']}\")\n",
    "        print(f\"Search results: {len(response['search_results'])}\")\n",
    "        \n",
    "        # Should be \"already_existed\" since we just ingested it\n",
    "        assert response[\"files_processed\"][0][\"ingestion_status\"] == \"already_existed\"\n",
    "        assert len(response[\"search_results\"]) > 0\n",
    "        \n",
    "        print(\" Existing document test passed!\")\n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Existing document test failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test MMR vs non-MMR results\n",
    "def test_mmr_comparison():\n",
    "    \"\"\"Compare results with and without MMR.\"\"\"\n",
    "    \n",
    "    base_request = {\n",
    "        \"client_id\": \"test_client_notebook\",\n",
    "        \"files\": [{\n",
    "            \"file_id\": \"test_doc_001\",\n",
    "            \"file_type\": \"txt\",\n",
    "            \"file_path\": str(test_file),\n",
    "            \"filename\": \"workplace_mental_health.txt\"\n",
    "        }],\n",
    "        \"question\": \"workplace mental health support resources\",\n",
    "        \"top_k\": 5\n",
    "    }\n",
    "    \n",
    "    # Test without MMR\n",
    "    request_no_mmr = {**base_request, \"mmr\": False}\n",
    "    response_no_mmr = search_or_ingest(request_no_mmr)\n",
    "    \n",
    "    # Test with MMR\n",
    "    request_mmr = {**base_request, \"mmr\": True}\n",
    "    response_mmr = search_or_ingest(request_mmr)\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*50)\n",
    "    print(\"MMR COMPARISON\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(\"\\\\nWithout MMR - Top 3 results:\")\n",
    "    for i, result in enumerate(response_no_mmr[\"search_results\"][:3]):\n",
    "        print(f\"{i+1}. Score: {result['score']:.4f}\")\n",
    "        print(f\"   Excerpt: {result['excerpt'][:100]}...\")\n",
    "        print()\n",
    "    \n",
    "    print(\"\\\\nWith MMR - Top 3 results:\")\n",
    "    for i, result in enumerate(response_mmr[\"search_results\"][:3]):\n",
    "        print(f\"{i+1}. Score: {result['score']:.4f}\")\n",
    "        print(f\"   Excerpt: {result['excerpt'][:100]}...\")\n",
    "        print()\n",
    "    \n",
    "    return response_no_mmr, response_mmr\n",
    "\n",
    "# Test multiple files\n",
    "def test_multiple_files():\n",
    "    \"\"\"Test with multiple files.\"\"\"\n",
    "    \n",
    "    # Create a second test document\n",
    "    test_content_2 = \"\"\"\n",
    "    Employee Benefits Overview\n",
    "    \n",
    "    Our comprehensive benefits package includes:\n",
    "    - Health insurance coverage\n",
    "    - Dental and vision plans\n",
    "    - Retirement savings programs\n",
    "    - Paid time off policies\n",
    "    - Professional development opportunities\n",
    "    - Flexible work arrangements\n",
    "    - Employee wellness programs\n",
    "    \n",
    "    We believe in supporting our employees' overall well-being and professional growth.\n",
    "    \"\"\"\n",
    "    \n",
    "    test_file_2 = INDEX_ROOT / \"test_benefits.txt\"\n",
    "    test_file_2.write_text(test_content_2, encoding='utf-8')\n",
    "    \n",
    "    request = {\n",
    "        \"client_id\": \"test_client_notebook\",\n",
    "        \"files\": [\n",
    "            {\n",
    "                \"file_id\": \"test_doc_001\",\n",
    "                \"file_type\": \"txt\",\n",
    "                \"file_path\": str(test_file),\n",
    "                \"filename\": \"workplace_mental_health.txt\"\n",
    "            },\n",
    "            {\n",
    "                \"file_id\": \"test_doc_002\",\n",
    "                \"file_type\": \"txt\",\n",
    "                \"file_path\": str(test_file_2),\n",
    "                \"filename\": \"employee_benefits.txt\"\n",
    "            }\n",
    "        ],\n",
    "        \"question\": \"What benefits and support does the company provide to employees?\",\n",
    "        \"top_k\": 6,\n",
    "        \"mmr\": False\n",
    "    }\n",
    "    \n",
    "    print(\"\\\\nTesting with multiple files...\")\n",
    "    \n",
    "    try:\n",
    "        response = search_or_ingest(request)\n",
    "        \n",
    "        print(f\"Files processed: {len(response['files_processed'])}\")\n",
    "        for file_proc in response[\"files_processed\"]:\n",
    "            print(f\"  - {file_proc['filename']}: {file_proc['ingestion_status']} ({file_proc['chunks_indexed']} chunks)\")\n",
    "        \n",
    "        print(f\"\\\\nSearch results: {len(response['search_results'])}\")\n",
    "        for i, result in enumerate(response[\"search_results\"][:3]):\n",
    "            print(f\"{i+1}. File: {result['filename']} | Score: {result['score']:.4f}\")\n",
    "            print(f\"   Excerpt: {result['excerpt'][:80]}...\")\n",
    "            print()\n",
    "        \n",
    "        print(\" Multiple files test passed!\")\n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Multiple files test failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run additional tests\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"RUNNING ADDITIONAL TESTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test 1: Existing document\n",
    "existing_result = test_existing_document()\n",
    "\n",
    "# Test 2: MMR comparison  \n",
    "mmr_results = test_mmr_comparison()\n",
    "\n",
    "# Test 3: Multiple files\n",
    "multiple_files_result = test_multiple_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caf72e5",
   "metadata": {},
   "source": [
    "## 19. Utility Functions and Store Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112c902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions to inspect the vector store\n",
    "def inspect_vector_store():\n",
    "    \"\"\"Inspect the current state of the vector store.\"\"\"\n",
    "    \n",
    "    store = VectorStore.get_shared_store(INDEX_ROOT)\n",
    "    \n",
    "    if not store.index_exists():\n",
    "        print(\"Vector store does not exist yet.\")\n",
    "        return\n",
    "    \n",
    "    store.load()\n",
    "    \n",
    "    print(\"Vector Store Inspection:\")\n",
    "    print(f\"  - Storage directory: {store.index_dir}\")\n",
    "    print(f\"  - Total chunks: {len(store._records)}\")\n",
    "    print(f\"  - Embedding dimension: {store._embedding_store.dimension}\")\n",
    "    print(f\"  - Embedding count: {store._embedding_store.count}\")\n",
    "    \n",
    "    # Group by client and file\n",
    "    file_stats = {}\n",
    "    for (client_id, file_id), indices in store._by_file.items():\n",
    "        key = f\"{client_id}::{file_id}\"\n",
    "        file_stats[key] = len(indices)\n",
    "    \n",
    "    print(f\"  - Files indexed: {len(file_stats)}\")\n",
    "    for file_key, chunk_count in file_stats.items():\n",
    "        print(f\"    * {file_key}: {chunk_count} chunks\")\n",
    "    \n",
    "    return store\n",
    "\n",
    "def list_all_chunks():\n",
    "    \"\"\"List all chunks in the store with their metadata.\"\"\"\n",
    "    \n",
    "    store = VectorStore.get_shared_store(INDEX_ROOT)\n",
    "    \n",
    "    if not store.index_exists():\n",
    "        print(\"Vector store does not exist yet.\")\n",
    "        return\n",
    "    \n",
    "    store.load()\n",
    "    \n",
    "    print(f\"All Chunks ({len(store._records)} total):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, record in enumerate(store._records[:10]):  # Show first 10\n",
    "        print(f\"Chunk {i+1}:\")\n",
    "        print(f\"  Client ID: {record.metadata.client_id}\")\n",
    "        print(f\"  File ID: {record.metadata.file_id}\")\n",
    "        print(f\"  Filename: {record.metadata.filename}\")\n",
    "        print(f\"  Page: {record.metadata.page_no}\")\n",
    "        print(f\"  Chunk ID: {record.metadata.chunk_id}\")\n",
    "        print(f\"  Text (first 100 chars): {record.text[:100]}...\")\n",
    "        print()\n",
    "    \n",
    "    if len(store._records) > 10:\n",
    "        print(f\"... and {len(store._records) - 10} more chunks\")\n",
    "\n",
    "def search_direct(query: str, client_id: str = \"test_client_notebook\", file_id: str = \"test_doc_001\", k: int = 3):\n",
    "    \"\"\"Direct search function for testing.\"\"\"\n",
    "    \n",
    "    store = VectorStore.get_shared_store(INDEX_ROOT)\n",
    "    \n",
    "    if not store.index_exists():\n",
    "        print(\"Vector store does not exist yet.\")\n",
    "        return []\n",
    "    \n",
    "    store.load()\n",
    "    embedder = SentenceTransformerBackend(model_name=EMBEDDING_MODEL)\n",
    "    \n",
    "    results = store.semantic_search_filtered(\n",
    "        query=query,\n",
    "        client_id=client_id,\n",
    "        file_id=file_id,\n",
    "        embedder=embedder,\n",
    "        k=k,\n",
    "        mmr=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Direct Search Results for: '{query}'\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"Result {i+1}:\")\n",
    "        print(f\"  Score: {result['score']:.4f}\")\n",
    "        print(f\"  File: {result['filename']}\")\n",
    "        print(f\"  Page: {result['page_no']}\")\n",
    "        print(f\"  Excerpt: {result['excerpt'][:150]}...\")\n",
    "        print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run store inspection\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"VECTOR STORE INSPECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "store_info = inspect_vector_store()\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"DIRECT SEARCH TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test direct search\n",
    "search_results = search_direct(\"mental health support resources\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9163bf",
   "metadata": {},
   "source": [
    "## 20. Conclusion and Summary\n",
    "\n",
    "This notebook provides a comprehensive implementation of the RAG pipeline using the `search_or_ingest` endpoint. Here's what we've accomplished:\n",
    "\n",
    "###  Key Components Implemented:\n",
    "\n",
    "1. **Complete Vector Store**: FAISS-based implementation with document and embedding stores\n",
    "2. **Document Processing**: Support for PDF, DOCX, and TXT files with text chunking\n",
    "3. **Embedding Generation**: SentenceTransformer-based embedding with caching\n",
    "4. **Search & Retrieval**: Semantic search with MMR (Maximal Marginal Relevance) support\n",
    "5. **Unified Endpoint**: Search-or-ingest functionality that handles both operations seamlessly\n",
    "\n",
    "###  Features Included:\n",
    "\n",
    "- **File Validation**: Comprehensive validation for file types and paths\n",
    "- **Chunk Management**: Intelligent text chunking with overlap for better context\n",
    "- **Concurrent Safety**: File locking and worker synchronization for safe concurrent operations\n",
    "- **Error Handling**: Robust error handling throughout the pipeline\n",
    "- **Metadata Tracking**: Complete metadata tracking for chunks and files\n",
    "- **Incremental Updates**: Append-only operations for efficient storage\n",
    "\n",
    "###  Testing Scenarios:\n",
    "\n",
    "- Single file ingestion and search\n",
    "- Existing file handling (skip re-ingestion)\n",
    "- Multiple file processing\n",
    "- MMR vs standard search comparison\n",
    "- Direct vector store operations\n",
    "\n",
    "###  Usage Summary:\n",
    "\n",
    "The `search_or_ingest` function is the main entry point that:\n",
    "1. Checks if files are already ingested (based on file hash)\n",
    "2. Ingests new files if needed\n",
    "3. Performs semantic search across all successfully processed files\n",
    "4. Returns combined results with file processing status\n",
    "\n",
    "This implementation provides a complete, production-ready RAG pipeline that can be easily integrated into larger applications or used standalone for document search and retrieval tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
